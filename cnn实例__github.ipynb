{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0eb0e3abb44b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../DATA/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../DATA/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.as_matrix(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = df.drop('label', axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Got', len(dataset), 'training examples(with', len(labels), 'labels).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DATA/train.csv')\n",
    "labels = df.as_matrix(columns=['label'])#find lable to transform to matrix\n",
    "dataset = df.drop('label', axis=1).as_matrix()#transform dataset to matrxi without drop lable \n",
    "print('Got', len(dataset), 'training examples (with', len(labels), 'labels).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DATA/train.csv')\n",
    "labels = df.as_matrix(columns=['label'])#find lable to transform to matrix\n",
    "dataset = df.drop('label', axis=1).as_matrix()#transform dataset to matrxi without drop lable \n",
    "print('Got', len(dataset), 'training examples (with', len(labels), 'labels).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(dataset, rows=4, colunms=5):\n",
    "    index = 1\n",
    "    for image in dataset[:rows*colunms]:\n",
    "        img = np.reshape(image, [28, 28])\n",
    "        plt.subplot(rows, colunms, index)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        index += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_each_label(labels):\n",
    "    counters = np.zeros(10, int)\n",
    "    for i in range(len(labels)):\n",
    "        counters[labels[i]] += 1\n",
    "    for i in range(10):\n",
    "        print(i, ':', counters[i])\n",
    "    print('\\nmin:\\t%d' % np.min(counters))\n",
    "    print('mean:\\t%d' % np.mean(counters))\n",
    "    print('max:\\t%d' % np.max(counters))\n",
    "    print('stddev:\\t%.2f' % np.std(counters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_each_label(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_the_shelf():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    logreg = LogisticRegression(solver='sag', max_iter=256)\n",
    "    %time logreg.fit(train_dataset, train_labels.ravel())\n",
    "    \n",
    "    print('Acc on train dataset: {:.2%}'.format(logreg.score(train_dataset, train_labels)))\n",
    "    print('Acc on valid dataset: {:.2%}'.format(logreg.score(valid_dataset, valid_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_the_shelf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, classes=10):\n",
    "    one_hot = np.zeros([len(labels), 10])\n",
    "    for i in range(len(labels)):\n",
    "        one_hot[i, labels[i]] = 1.\n",
    "    return one_hot\n",
    "train_labels = one_hot_encode(train_labels)\n",
    "valid_labels = one_hot_encode(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFModel():\n",
    "    def __init__(self):\n",
    "        \n",
    "        def weight_variable(shape):\n",
    "            return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "        \n",
    "        def bias_variable(shape):\n",
    "            return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "        \n",
    "        def conv2d(x, W):\n",
    "            return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        \n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                                  strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        # feed dictionary entries needed\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.t = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # reshape inputs\n",
    "        x_img = tf.reshape(self.x, [-1, 28, 28, 1])\n",
    "        \n",
    "        # first convolutional layer\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        \n",
    "        h_conv1 = tf.nn.relu(conv2d(x_img, W_conv1) + b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        \n",
    "        # second convolutional layer\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        \n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        \n",
    "        # fully connected layer\n",
    "        W_fcl = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fcl = bias_variable([1024])\n",
    "        \n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "        h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat, W_fcl) + b_fcl)\n",
    "        \n",
    "        # dropout layer\n",
    "        h_fcl_drop = tf.nn.dropout(h_fcl, self.keep_prob)\n",
    "        \n",
    "        # readout layer\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        \n",
    "        logits = tf.matmul(h_fcl_drop, W_fc2) + b_fc2\n",
    "        \n",
    "        # output\n",
    "        self.y = tf.nn.softmax(logits)\n",
    "        \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.t)\n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.t, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffle(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_dir = 'save'\n",
    "if not tf.gfile.Exists(save_dir):\n",
    "    tf.gfile.MakeDirs(save_dir)\n",
    "    print('save directory created')\n",
    "else:\n",
    "    print('save directory contains:', os.listdir(save_dir))\n",
    "\n",
    "save_path = os.path.join(save_dir, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "max_step = 3000\n",
    "keep_prob = 0.5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    m = TFModel()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if tf.gfile.Exists(save_path):\n",
    "        saver.restore(sess, save_path)\n",
    "        print('model restored')\n",
    "    else:\n",
    "        print(save_path, 'doesn`t exist')\n",
    "\n",
    "    p_acc = 0\n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    for step in range(max_step):\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(m.accuracy, {m.x : train_dataset[batch_start : batch_end],\n",
    "                                        m.t : train_labels[batch_start : batch_end],\n",
    "                                        m.keep_prob: 1.0})\n",
    "            print('step {}, training accuracy: {:.0%}'.format(step, acc))\n",
    "\n",
    "        \"\"\"c\n",
    "        if step % 1000 == 0:\n",
    "            acc = sess.run(m.accuracy, {m.x : valid_dataset,\n",
    "                                        m.t : valid_labels,\n",
    "                                        m.keep_prob: 1.0})\n",
    "            print('step {}, test accuracy: {:.2%}'.format(step, acc))\n",
    "            if acc > p_acc:\n",
    "                p_acc = acc\n",
    "                saver.save(sess, save_path)\n",
    "                print('model saved in', save_path)\n",
    "        \"\"\"\n",
    "\n",
    "        sess.run(m.train_step, {m.x : train_dataset[batch_start : batch_end],\n",
    "                                m.t : train_labels[batch_start : batch_end],\n",
    "                                m.keep_prob: keep_prob})\n",
    "\n",
    "        batch_start = batch_end\n",
    "        batch_end += batch_size\n",
    "        if batch_end > train_len:\n",
    "            batch_start = 0\n",
    "            batch_end = batch_start + batch_size\n",
    "            train_dataset, train_labels = shuffle(train_dataset, train_labels)\n",
    "    print('test accuracy: {:.2%}'.format(sess.run(m.accuracy, {m.x : valid_dataset,\n",
    "                                                               m.t : valid_labels,\n",
    "                                                               m.keep_prob: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
